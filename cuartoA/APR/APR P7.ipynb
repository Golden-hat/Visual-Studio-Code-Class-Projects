{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de la regularización es **evitar modelos sobreajustados modificando el comportamiento de descenso por gradiente, objetivo y datos.**\n",
    "\n",
    "La técnica básica para regularizar es evaluar la bondad de cualquier modificación mediante estimación del rendimiento teórico (en validación). Esto se puede hacer mediante diversos métodos.\n",
    "\n",
    "Cargamos el dataset como se ha hecho en prácticas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:29:46.660910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 19:29:46.673085: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 19:29:46.676500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; import keras_tuner\n",
    "\n",
    "keras.utils.set_random_seed(23); input_dim = (28, 28, 1); num_classes = 10\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train_val = np.expand_dims(x_train_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(x_train_val.shape, y_train_val.shape, x_test.shape, y_test.shape)\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez el hipermodelo incorpora capas convolucionales y de pooling.\n",
    "\n",
    "Two convolutional layers with:\n",
    "\n",
    "- Filters: filters in the first layer, 2*filters in the second.\n",
    "- Kernel size: (3, 3)\n",
    "- Activation: ReLU\n",
    "\n",
    "Max pooling layers reduce spatial dimensions by (2, 2).\n",
    "\n",
    "Esto es equivalente a la anterior práctica. Sin embargo esta vez tenemos también capas encargadas de rotar, trasladar, y hacer zoom a las entradas:\n",
    "\n",
    "```python\n",
    "    M.add(keras.layers.RandomRotation(factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.RandomTranslation(factor, factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.RandomZoom(factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.Rescaling(1./255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "  def build(self, hp):\n",
    "    M = keras.Sequential()\n",
    "    M.add(keras.Input(shape=(28, 28, 1)))\n",
    "    factor = hp.Float(\"factor\", min_value=0.01, max_value=0.3, step=2, sampling=\"log\")\n",
    "    M.add(keras.layers.RandomRotation(factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.RandomTranslation(factor, factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.RandomZoom(factor, fill_mode=\"nearest\"))\n",
    "    M.add(keras.layers.Rescaling(1./255))\n",
    "\n",
    "    filters = 64\n",
    "    M.add(keras.layers.Conv2D(filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    M.add(keras.layers.Conv2D(2*filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    M.add(keras.layers.Flatten())\n",
    "    M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "\n",
    "    # dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1)\n",
    "    dropout = 0.5\n",
    "    M.add(keras.layers.Dropout(dropout))\n",
    "    M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00015)\n",
    "\n",
    "    M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return M\n",
    "  def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "    factor = 0.32; patience = 5\n",
    "\n",
    "    reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=factor, patience=patience, min_delta=1e-4, min_lr=1e-5)\n",
    "\n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=1e-5)\n",
    "\n",
    "    kwargs['callbacks'].extend([reduce_cb, early_cb])\n",
    "    return M.fit(x, y, batch_size=256, epochs=100, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de cómputo de este entrenamiento es muy pesado (por eso solo vamos a hacer una trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "MyHyperModel(), objective=\"val_accuracy\", max_trials=1, executions_per_trial=1,\n",
    "overwrite=True, directory=\"/tmp\", project_name=\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 28m 34s]\n",
      "val_accuracy: 0.8572999835014343\n",
      "\n",
      "Best val_accuracy So Far: 0.8572999835014343\n",
      "Total elapsed time: 00h 28m 34s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/MNIST\n",
      "Showing 1 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "factor: 0.04\n",
      "Score: 0.8572999835014343\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassin/anaconda3/envs/per/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Hyperparameters: {'factor': 0.04} Loss: 0.4045 Precisión: 85.46%\n"
     ]
    }
   ],
   "source": [
    "num_models = 1\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=num_models)\n",
    "best_models = tuner.get_best_models(num_models=num_models)\n",
    "for m in range(num_models):\n",
    "  values = best_hyperparameters[m].values\n",
    "  score = best_models[m].evaluate(x_test, y_test, verbose=0)\n",
    "  print(f'Model {m}: Hyperparameters: {values!s} Loss: {score[0]:.4} Precisión: {score[1]:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
