{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialización de los pesos de una red\n",
    "\n",
    "Veamos un ejemplo (es el que saldrá en el examen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion-MNIST con inicialización RandomNormal\n",
    "- MLP inicial: MLP con una capa oculta de 800 RELUs, batch size 16, 20 épocas; 88.0% en test\n",
    "- Mejor arquitectura: una capa oculta de 800 RELUs, 89.0% en val, 88.3% en test ( 88.0% modelo val)\n",
    "- Learning rate y batch size: ajustados a 0.00015 y 256; 89.6% en val, 89.8% en test (89.1% modelo val)\n",
    "- ReduceLROnPlateau: factor 0.32 y paciencia 5; en val, 89.6% en test (89.5% modelo val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hacemos la inicialización de la semilla, la librería, la partición del datasey y la lectura del set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del Código\n",
    "\n",
    "Este script de Python es una preparación previa para entrenar una red neuronal en el conjunto de datos Fashion MNIST utilizando Keras y Keras Tuner\n",
    "\n",
    "1. **Importación de Librerías**:\n",
    "   - `numpy` y `matplotlib.pyplot`: Librerías comúnmente usadas para operaciones numéricas y visualización.\n",
    "   - `os`: Se utiliza para establecer variables de entorno y suprimir los mensajes extensos de TensorFlow.\n",
    "   - `keras` y `keras_tuner`: Para construir, entrenar y ajustar modelos de redes neuronales.\n",
    "\n",
    "2. **Configuración de Semilla Aleatoria**:\n",
    "   - `keras.utils.set_random_seed(23)`: Establece una semilla aleatoria para garantizar la reproducibilidad de los resultados.\n",
    "\n",
    "3. **Definición de Dimensiones de Entrada y Clases**:\n",
    "   - `input_dim = 784`: Representa la dimensión de entrada (28x28 píxeles convertidos en un vector de 784 valores).\n",
    "   - `num_classes = 10`: Representa las 10 clases en las que se clasifica Fashion MNIST.\n",
    "\n",
    "4. **Carga de Datos**:\n",
    "   - `keras.datasets.fashion_mnist.load_data()`: Carga el conjunto de datos Fashion MNIST, dividido en datos de entrenamiento y prueba.\n",
    "\n",
    "5. **Normalización y Redimensionamiento**:\n",
    "   - `x_train_val` y `x_test`: Se redimensionan los datos de imágenes de 28x28 a vectores de 784 y se normalizan dividiendo entre 255.0 para escalar los valores entre 0 y 1.\n",
    "\n",
    "6. **Codificación One-Hot de Etiquetas**:\n",
    "   - `keras.utils.to_categorical`: Convierte las etiquetas en formato one-hot, necesario para el entrenamiento de la red neuronal.\n",
    "\n",
    "7. **División de Datos de Validación**:\n",
    "   - Se dividen los datos de entrenamiento en dos conjuntos:\n",
    "     - `x_train` y `y_train`: Conjunto de entrenamiento (todas las muestras excepto las últimas 10,000).\n",
    "     - `x_val` y `y_val`: Conjunto de validación (las últimas 10,000 muestras).\n",
    "\n",
    "### Resultado:\n",
    "Este código deja los datos listos para entrenar un modelo de red neuronal, con conjuntos de datos normalizados y divididos en entrenamiento, validación y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 18:15:12.053716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 18:15:12.187709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 18:15:12.231693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; import keras_tuner\n",
    "\n",
    "keras.utils.set_random_seed(23); input_dim = 784; num_classes = 10\n",
    "\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativas de hipermodelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploramos inicialización RandomNormal con desviación estándar entre 0.01 y 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "  def build(self, hp):\n",
    "    M = keras.Sequential()\n",
    "    M.add(keras.Input(shape=(784,)))\n",
    "\n",
    "    # La desviación estándar viene dada entre min_value y max_value\n",
    "    stddev = hp.Float(\"stddev\", min_value=0.01, max_value=0.10)\n",
    "    kernel_initializer = keras.initializers.RandomNormal(stddev=stddev)\n",
    "\n",
    "    M.add(keras.layers.Dense(units=800, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00015)\n",
    "\n",
    "    M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return M\n",
    "\n",
    "  def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "    factor = 0.32; patience = 5\n",
    "    \n",
    "    reduce_cb = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=factor, patience=patience, min_delta=1e-4, min_lr=1e-5)\n",
    "    \n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=1e-5)\n",
    "    \n",
    "    kwargs['callbacks'].extend([reduce_cb, early_cb])\n",
    "    return M.fit(x, y, batch_size=256, epochs=100, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate inicial y final con 10 decay_steps y power por omisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "  def build(self, hp):\n",
    "    M = keras.Sequential()\n",
    "    M.add(keras.Input(shape=(784,)))\n",
    "    M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "    M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    initial_learning_rate = hp.Float(\"initial_learning rate\", min_value=0.2983, max_value=0.3200)\n",
    "    end_learning_rate = hp.Float(\"end_learning rate\", min_value=0.2800, max_value=0.2983)\n",
    "    \n",
    "    decay_steps = 10\n",
    "    \n",
    "    lr_schedule = keras.optimizers.schedules.PolynomialDecay(initial_learning_rate,\n",
    "    decay_steps, end_learning_rate)\n",
    "    \n",
    "    opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return M\n",
    "\n",
    "  def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "    patience = 10\n",
    "    \n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=0.0)\n",
    "    \n",
    "    kwargs['callbacks'].append(early_cb)\n",
    "    return M.fit(x, y, batch_size=256, epochs=100, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploramos learning rate, momentum y Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "  def build(self, hp):\n",
    "    M = keras.Sequential()\n",
    "    M.add(keras.Input(shape=(784,)))\n",
    "    M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "    M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    learning_rate = hp.Float(\"learning rate\", min_value=0.25, max_value=0.35)\n",
    "    momentum = hp.Float(\"momentum\", min_value=0.1, max_value=0.2)\n",
    "    nesterov = hp.Boolean(\"nesterov\")\n",
    "    \n",
    "    opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "    \n",
    "    M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return M\n",
    "\n",
    "  def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "    factor = 0.3787; patience = 10\n",
    "    reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=factor, patience=patience, min_delta=1e-5, min_lr=0.0)\n",
    "    \n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=1e-5)\n",
    "    \n",
    "    kwargs['callbacks'].extend([reduce_cb, early_cb])\n",
    "    return M.fit(x, y, batch_size=256, epochs=100, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hacemos una evaluación en test del mejor modelo en validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "MyHyperModel(), objective=\"val_accuracy\", max_trials=10, executions_per_trial=1,\n",
    "overwrite=True, directory=\"/tmp\", project_name=\"Fashion-MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 22s]\n",
      "val_accuracy: 0.8970999717712402\n",
      "\n",
      "Best val_accuracy So Far: 0.900600016117096\n",
      "Total elapsed time: 00h 17m 05s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/Fashion-MNIST\n",
      "Showing 1 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "stddev: 0.07271143839834814\n",
      "Score: 0.900600016117096\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yassin/anaconda3/envs/per/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3103\n",
      "Precisión: 89.55%\n"
     ]
    }
   ],
   "source": [
    "best = tuner.get_best_models(num_models=1)[0]\n",
    "score = best.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Loss: {score[0]:.4}\\nPrecisión: {score[1]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
