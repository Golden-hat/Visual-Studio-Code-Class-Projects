{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capas convolucionales y de agrupación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset como se ha hecho en prácticas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; import keras_tuner\n",
    "\n",
    "keras.utils.set_random_seed(23); input_dim = (28, 28, 1); num_classes = 10\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train_val = np.expand_dims(x_train_val, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(x_train_val.shape, y_train_val.shape, x_test.shape, y_test.shape)\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez el hipermodelo incorpora capas convolucionales y de pooling.\n",
    "\n",
    "Two convolutional layers with:\n",
    "\n",
    "- Filters: filters in the first layer, 2*filters in the second.\n",
    "- Kernel size: (3, 3)\n",
    "- Activation: ReLU\n",
    "\n",
    "Max pooling layers reduce spatial dimensions by (2, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "  def build(self, hp):\n",
    "    M = keras.Sequential()\n",
    "    M.add(keras.Input(shape=(28, 28, 1)))\n",
    "    filters = hp.Int(\"filters\", min_value=8, max_value=64, step=2, sampling=\"log\")\n",
    "\n",
    "    M.add(keras.layers.Conv2D(filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    M.add(keras.layers.Conv2D(2*filters, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    M.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    M.add(keras.layers.Flatten())\n",
    "    M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "    M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00168)\n",
    "    \n",
    "    M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return M\n",
    "  def fit(self, hp, M, x, y, xy_val, **kwargs):\n",
    "    factor = 0.3787; patience = 5\n",
    "\n",
    "    reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=factor, patience=patience, min_delta=1e-4, min_lr=1e-5)\n",
    "\n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2*patience, min_delta=1e-5)\n",
    "\n",
    "    kwargs['callbacks'].extend([reduce_cb, early_cb])\n",
    "    return M.fit(x, y, batch_size=256, epochs=100, validation_data=xy_val, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de cómputo de este entrenamiento es muy pesado (por eso solo vamos a hacer una trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "MyHyperModel(), objective=\"val_accuracy\", max_trials=1, executions_per_trial=1,\n",
    "overwrite=True, directory=\"/tmp\", project_name=\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 02m 03s]\n",
      "val_accuracy: 0.9114000201225281\n",
      "\n",
      "Best val_accuracy So Far: 0.9114000201225281\n",
      "Total elapsed time: 00h 02m 03s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /tmp/MNIST\n",
      "Showing 1 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "filters: 8\n",
      "Score: 0.9114000201225281\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0: Hyperparameters: {'filters': 8} Loss: 0.5079 Precisión: 90.70%\n"
     ]
    }
   ],
   "source": [
    "num_models = 1\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=num_models)\n",
    "best_models = tuner.get_best_models(num_models=num_models)\n",
    "for m in range(num_models):\n",
    "  values = best_hyperparameters[m].values\n",
    "  score = best_models[m].evaluate(x_test, y_test, verbose=0)\n",
    "  print(f'Model {m}: Hyperparameters: {values!s} Loss: {score[0]:.4} Precisión: {score[1]:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
