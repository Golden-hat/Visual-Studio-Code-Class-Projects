{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exámenes A1 y A3 de APR en el grupo 4CO21, turno 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen A1: ejercicio con Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ejercicios de las sesiones de laboratorio del bloque 1 se han basado en el corpus Fashion-MNIST. Hemos utilizado el código siguiente para leer Fashion-MNIST con su partición train-test estándar, normalizando las imágenes a $\\,[0,1]\\,$, y estableciendo una partición train-val-test mediante partición del train estándar en train-val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (50000, 784) (50000, 10) val (10000, 784) (10000, 10) test (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'; import keras\n",
    "input_dim = 784; num_classes = 10\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]\n",
    "print(f'train {x_train.shape} {y_train.shape} val {x_val.shape} {y_val.shape} test {x_test.shape} {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a utilizar la arquitectura de MLP con una capa oculta de $\\,800\\,$ RELUs;  optimizador Adam por defecto, batch size $\\,256;\\;$ planificador ReduceLROnPlateau con factor $\\,0.32\\,$ y paciencia $\\,5;\\;$ y regularización mediante early stopping con paciencia $\\,10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(optimizer=\"adam\"):\n",
    "  M = keras.Sequential()\n",
    "  M.add(keras.Input(shape=(784,)))\n",
    "  M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "  M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "  M.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "  reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.32, patience=5, restore_best_weights=True)\n",
    "  early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-5)\n",
    "  M.fit(x_train, y_train, batch_size=256, epochs=100, verbose=0, validation_data=(x_val, y_val),\n",
    "    callbacks=[reduce_cb, early_cb])\n",
    "  _, acc = M.evaluate(x_test, y_test, verbose=0)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El optimizador Adam que venimos utilizando proporciona una precisión de $\\,89.90\\%,\\,$ cuando repetimos el experimento 5 veces, dado que cada ejecución inicializada con pesos aleatorios proporciona resultados diferentes y necesitamos promediar. Sin embargo, existen [optimizadores alternativos](https://keras.io/api/optimizers/) como el [optimizador AdamW](https://keras.io/api/optimizers/adamw/) que podemos evaluar de la misma manera:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión media: 89.96%  Desviación estándar: 0.12%\n",
      "Tiempo (hh:mm:ss): 00:01:19\n"
     ]
    }
   ],
   "source": [
    "import time; start = time.time()\n",
    "keras.utils.set_random_seed(seed=23);\n",
    "\n",
    "num_exp = 5; acc = np.zeros(num_exp)\n",
    "for exp in range(num_exp):\n",
    "    acc[exp] = run_exp(optimizer=\"adamw\")\n",
    "print(f'Precisión media: {acc.mean():.2%}  Desviación estándar: {acc.std():.2%}')\n",
    "print('Tiempo (hh:mm:ss):', time.strftime('%H:%M:%S', time.gmtime(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio (2 puntos):** $\\;$ escoge otro [optimizador](https://keras.io/api/optimizers/) repitiendo el experimento 5 veces para realizar la estimación de la precisión en el conjunto de test como en el experimento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (50000, 784) (50000, 10) val (10000, 784) (10000, 10) test (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'; import keras\n",
    "input_dim = 784; num_classes = 10\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train_val = x_train_val.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, input_dim).astype(\"float32\") / 255.0\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]\n",
    "print(f'train {x_train.shape} {y_train.shape} val {x_val.shape} {y_val.shape} test {x_test.shape} {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select Adamax\n",
    "def run_exp(optimizer=\"adamax\"):\n",
    "  M = keras.Sequential()\n",
    "  M.add(keras.Input(shape=(784,)))\n",
    "  M.add(keras.layers.Dense(units=800, activation='relu'))\n",
    "  M.add(keras.layers.Dense(10, activation='softmax'))\n",
    "  M.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "  reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.32, patience=5, restore_best_weights=True)\n",
    "  early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-5)\n",
    "  M.fit(x_train, y_train, batch_size=256, epochs=100, verbose=0, validation_data=(x_val, y_val),\n",
    "    callbacks=[reduce_cb, early_cb])\n",
    "  _, acc = M.evaluate(x_test, y_test, verbose=1)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - accuracy: 0.8961 - loss: 0.3179\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - accuracy: 0.8995 - loss: 0.3076\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - accuracy: 0.8972 - loss: 0.3115\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.8959 - loss: 0.3189\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.8953 - loss: 0.3208\n",
      "Precisión media: 89.65%  Desviación estándar: 0.06%\n",
      "Tiempo (hh:mm:ss): 00:01:49\n"
     ]
    }
   ],
   "source": [
    "import time; start = time.time()\n",
    "keras.utils.set_random_seed(seed=23);\n",
    "\n",
    "num_exp = 5; acc = np.zeros(num_exp)\n",
    "for exp in range(num_exp):\n",
    "    # We select adamax\n",
    "    acc[exp] = run_exp(optimizer=\"adamax\")\n",
    "print(f'Precisión media: {acc.mean():.2%}  Desviación estándar: {acc.std():.2%}')\n",
    "print('Tiempo (hh:mm:ss):', time.strftime('%H:%M:%S', time.gmtime(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen A3: ejercicio con CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las dos primeras sesiones de laboratorio del bloque 2 seguimos con MNIST y Fashion-MNIST; vimos que con CNNs sencillas convenientemente regularizadas se obtenían precisiones en test muy buenas, del $\\,99.5\\%\\,$ en MNIST y $\\,92.0\\%\\,$ en Fashion-MNIST. A partir de la tercera sesión de laboratorio del bloque 2 utilizamos el corpus de imágenes a color CIFAR-10. El siguiente código lee CIFAR-10 con su partición train-test estándar y establece una partición train-val-test mediante partición del train estándar en train-val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000, 10) (10000, 32, 32, 3) (10000, 10) (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; from keras import layers\n",
    "keras.utils.set_random_seed(23)\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train_val = x_train_val.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tercera sesión del bloque 2 (sesión 8) vimos que una CNN sencilla obtenía precisiones alrededor del $73\\%$. A continuación se define una función para realizar un experimento con una CNN incluso más sencilla. La normalización de imágenes, a $\\,[-1, 1],\\,$ se integra como primera capa tras la de entrada. Tras la normalización, la red aplica dos capas convolucionales de 32 y 64 filtros de $\\,5\\times 5$ y $\\,3\\times 3$, respectivamente. Cada capa convolucional viene seguida por una capa de agrupación AveragePooling2D con ventana $2\\times 2.$ Tras los dos pares Conv2D-AveragePooling2D, la salida se aplana y se procesa mediante un MLP con 500 unidades seguida de una capa de regularización Dropout con probabilidad $\\,0.5\\,$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp2():\n",
    "  M = keras.Sequential()\n",
    "  M.add(keras.Input(shape=(32, 32, 3)))\n",
    "  M.add(layers.Rescaling(scale=1 / 127.5, offset=-1))\n",
    "  M.add(layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Flatten())\n",
    "  M.add(layers.Dense(units=500, activation='relu'))\n",
    "  M.add(layers.Dropout(0.5))\n",
    "  M.add(layers.Dense(10, activation='softmax'))\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.00015)\n",
    "  M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "  reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.32, patience=5, restore_best_weights=True)\n",
    "  early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-5)\n",
    "  M.fit(x_train, y_train, batch_size=256, epochs=100, verbose=0, validation_data=(x_val, y_val),\n",
    "    callbacks=[reduce_cb, early_cb])\n",
    "  _, acc = M.evaluate(x_test, y_test, verbose=0)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos `run_exp2` para comprobar que la CNN definida arriba obtiene una precisión próxima al $71\\%$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión media: 71.17% Desviación estándar: 0.81%\n",
      "Tiempo (hh:mm:ss): 00:06:20\n"
     ]
    }
   ],
   "source": [
    "import time; start = time.time()\n",
    "keras.utils.set_random_seed(seed=23); \n",
    "num_exp = 5; acc = np.zeros(num_exp)\n",
    "for exp in range(num_exp):\n",
    "    acc[exp] = run_exp2()\n",
    "print(f'Precisión media: {acc.mean():.2%} Desviación estándar: {acc.std():.2%}')\n",
    "print('Tiempo (hh:mm:ss):', time.strftime('%H:%M:%S', time.gmtime(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio (2 puntos):** $\\;$ Define una nueva función `run_exp3` para experimentar con la CNN definida arriba añadiendo una o más capas de aumento de datos de las vistas en las sesiones de laboratorio ([RandomFlip](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_flip/), [RandomTranslation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_translation/), [RandomRotation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_rotation/), etc.). Escoge las [capas de aumento de imágenes](https://keras.io/api/layers/preprocessing_layers/image_augmentation/]) que consideres prometedoras y aplícalas adecuadamente. Repite `run_exp3` al menos dos veces para estimar la precisión; deberías obtener un $73\\%$ al menos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 20:05:12.823629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733943912.842555  139956 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733943912.845997  139956 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000, 10) (10000, 32, 32, 3) (10000, 10) (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import matplotlib.pyplot as plt\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import keras; from keras import layers\n",
    "keras.utils.set_random_seed(23)\n",
    "(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train_val = x_train_val.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "y_train_val = keras.utils.to_categorical(y_train_val, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "x_train = x_train_val[:-10000]; x_val = x_train_val[-10000:]\n",
    "y_train = y_train_val[:-10000]; y_val = y_train_val[-10000:]\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp3():\n",
    "  M = keras.Sequential()\n",
    "  M.add(keras.Input(shape=(32, 32, 3)))\n",
    "  M.add(layers.Rescaling(scale=1 / 127.5, offset=-1))\n",
    "  # We add image aumentation layers. This performs our data augmentation\n",
    "  M.add(layers.RandomFlip(\"horizontal\"))\n",
    "  M.add(layers.RandomRotation(0.1))\n",
    "  M.add(layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Flatten())\n",
    "  M.add(layers.Dense(units=500, activation='relu'))\n",
    "  M.add(layers.Dropout(0.5))\n",
    "  M.add(layers.Dense(10, activation='softmax'))\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.00015)\n",
    "  M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "  reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.32, patience=5, restore_best_weights=True)\n",
    "  early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-5)\n",
    "  M.fit(x_train, y_train, batch_size=256, epochs=100, verbose=0, validation_data=(x_val, y_val),\n",
    "    callbacks=[reduce_cb, early_cb])\n",
    "  _, acc = M.evaluate(x_test, y_test, verbose=1)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733943919.765359  139956 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2616 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0000 00:00:1733943922.492902  141742 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7202 - loss: 0.8050\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6956 - loss: 0.8766\n",
      "Precisión media: 70.56% Desviación estándar: 1.32%\n",
      "Tiempo (hh:mm:ss): 00:07:06\n"
     ]
    }
   ],
   "source": [
    "import time; start = time.time()\n",
    "keras.utils.set_random_seed(seed=23); \n",
    "num_exp = 2; acc = np.zeros(num_exp)\n",
    "for exp in range(num_exp):\n",
    "    acc[exp] = run_exp3()\n",
    "print(f'Precisión media: {acc.mean():.2%} Desviación estándar: {acc.std():.2%}')\n",
    "print('Tiempo (hh:mm:ss):', time.strftime('%H:%M:%S', time.gmtime(time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intentemos obtener mejores resultados (no se consigue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp3():\n",
    "  M = keras.Sequential()\n",
    "  M.add(keras.Input(shape=(32, 32, 3)))\n",
    "  M.add(layers.Rescaling(scale=1 / 127.5, offset=-1))\n",
    "  M.add(layers.RandomFlip(\"horizontal\"))\n",
    "  # Añadimos RandomTranslation\n",
    "  M.add(layers.RandomTranslation(0.2, 0.2, fill_mode=\"nearest\"))\n",
    "  M.add(layers.Conv2D(64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "  M.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "  M.add(layers.Flatten())\n",
    "  M.add(layers.Dense(units=500, activation='relu'))\n",
    "  M.add(layers.Dropout(0.5))\n",
    "  M.add(layers.Dense(10, activation='softmax'))\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.00015)\n",
    "  M.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "  reduce_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.32, patience=5, restore_best_weights=True)\n",
    "  early_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, min_delta=1e-5)\n",
    "  M.fit(x_train, y_train, batch_size=256, epochs=100, verbose=0, validation_data=(x_val, y_val),\n",
    "    callbacks=[reduce_cb, early_cb])\n",
    "  _, acc = M.evaluate(x_test, y_test, verbose=1)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7019 - loss: 0.8566\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7047 - loss: 0.8563\n",
      "Precisión media: 70.14% Desviación estándar: 0.01%\n",
      "Tiempo (hh:mm:ss): 00:07:05\n"
     ]
    }
   ],
   "source": [
    "import time; start = time.time()\n",
    "keras.utils.set_random_seed(seed=23); \n",
    "num_exp = 2; acc = np.zeros(num_exp)\n",
    "for exp in range(num_exp):\n",
    "    acc[exp] = run_exp3()\n",
    "print(f'Precisión media: {acc.mean():.2%} Desviación estándar: {acc.std():.2%}')\n",
    "print('Tiempo (hh:mm:ss):', time.strftime('%H:%M:%S', time.gmtime(time.time() - start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "per",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
